# Real-Time E-Commerce Data Lake & Analytics Platform

An end-to-end **event-driven data lake and analytics system** built using **Kafka, Spring Boot, AWS S3, Glue, Athena, Airflow, and Metabase**.  
This project demonstrates a **production-style Bronze â†’ Silver â†’ Gold architecture** with orchestration, schema evolution, and BI dashboards.

---

## ğŸ“Œ Architecture Overview

**Flow**
Producers â†’ Kafka â†’ Spring Consumer â†’ S3 (Bronze)
â†’ AWS Glue Crawler
â†’ Athena CTAS (Silver)
â†’ Aggregations (Gold)
â†’ BI Dashboard (Metabase)


**Design Principles**
- Event-driven ingestion
- Immutable raw data (Bronze)
- Curated analytics-ready data (Silver)
- Business aggregates (Gold)
- Fully containerized local setup
- Cloud-native analytics (serverless)

---

## ğŸ§© Tech Stack

### Data Ingestion
- **Apache Kafka** â€“ Event streaming platform
- **Spring Boot (Java 17)** â€“ Kafka consumer, S3 writer
- **Python** â€“ Kafka event producer (mock data)

### Data Lake & Processing
- **Amazon S3** â€“ Bronze, Silver, Gold layers
- **AWS Glue Crawler** â€“ Schema discovery & cataloging
- **Amazon Athena** â€“ SQL-based analytics (CTAS)

### Orchestration
- **Apache Airflow (Dockerized)**  
  - Glue Crawlers  
  - Athena CTAS queries  
  - Dependency management & retries  

### BI & Analytics
- **Metabase** â€“ Dashboarding & visualization
- **Athena JDBC** â€“ Direct analytics on S3 data

### Platform & Tooling
- **Docker & Docker Compose**
- **AWS IAM (least privilege)**
- **GitHub-ready project structure**

---

## ğŸ“‚ Data Lake Layers

### ğŸ¥‰ Bronze (Raw)
- One folder per Kafka topic
- JSON events exactly as produced
- Immutable, append-only

---

### ğŸ¥ˆ Silver (Curated)
- Normalized schema
- Partitioned by `date` and `hour`
- Stored as **Parquet**
- Created using **Athena CTAS**

---

### ğŸ¥‡ Gold (Business Aggregates)

Examples:
- Orders Daily Summary
- Payments Daily Summary
- User Activity Summary

ecommerce_gold.orders_daily_summary
ecommerce_gold.payment_daily_summary
ecommerce_gold.user_daily_activity_summary

---

## ğŸ›  Airflow DAG

**Pipeline Steps**
1. Run Bronze Glue Crawler
2. Create Silver table (CTAS)
3. Run Silver Glue Crawler
4. Create Gold aggregate tables
5. Retry-safe & cost-aware execution

**Key Features**
- Deferrable Glue operators
- No duplicate crawler execution
- Cost-optimized retries
- Fully Dockerized

---

## ğŸ“Š Dashboards (Metabase)

Built dashboards include:
- Orders per day
- Revenue trends
- Payment success/failure ratio
- User activity metrics

Metabase connects directly to **Athena**, querying data stored in **S3 (Gold layer)**.

---

## ğŸ” Secrets & Configuration

Secrets are **never committed**.

### Used locally via:
- `.env` file
- Docker environment variables

### Protected using:
- `.gitignore`
- `.env.example` for reference

---

## ğŸš€ How to Run (High Level)

## ğŸš€ How to Run the Project (Step-by-Step)

Follow the steps **in order**. Each layer is started independently for clarity and control.

---

## ğŸ” Prerequisites

Before starting **any container**, you **must** provide AWS credentials.

Create a `.env` file (this file is **NOT committed to Git**):

```env
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
AWS_DEFAULT_REGION=eu-north-1
```
These credentials are required for:

- Writing to S3
- Running Glue Crawlers
- Executing Athena queries
- Connecting Metabase to Athena

1ï¸âƒ£ Start Kafka, Zookeeper, Producer & Spring Consumer
Go inside the main project directory:
cd datalake

Start core ingestion services:
```docker compose up -d```

This will start:

Zookeeper, Kafka, Python Kafka Producer, Spring Boot Kafka Consumer

Verify logs:
```
docker logs -f python-producer
docker logs -f spring-consumer
```

Verify Bronze Layer

Check your S3 bucket:

s3://ecommerce-event-bronze/


You should see topic-wise folders with JSON events.

2ï¸âƒ£ Start Apache Airflow (Orchestration)
Go inside the Airflow directory:
cd airflow

Start Airflow services:
```
docker compose -f docker-compose.airflow.yml up -d
```

Create Airflow Admin User:
```
docker compose -f docker-compose.airflow.yml run --rm airflow-webserver airflow users create \
  --username admin \
  --password admin \
  --firstname Admin \
  --lastname User \
  --role Admin \
  --email admin@example.com
```

Open Airflow UI:
http://localhost:8080/home

Login with:

Username: admin
Password: admin

3ï¸âƒ£ Run the Airflow DAG

Open Airflow UI

Enable the DAG:

ecommerce_bronze_silver_gold_pipeline


Trigger the DAG manually

What the DAG does:

Runs Bronze Glue Crawler

Creates Silver table (Athena CTAS)

Runs Silver Glue Crawler

Creates Gold aggregate tables

Verify in AWS:

Glue Catalog â†’ Databases & Tables

Athena â†’ ecommerce_silver and ecommerce_gold

4ï¸âƒ£ Start Metabase (BI Dashboard)
Go inside the Metabase directory:
cd metabase

Start Metabase:
```
docker compose -f docker-compose.metabase.yml up -d
```

Open Metabase UI:
http://localhost:3000

5ï¸âƒ£ Connect Metabase to Amazon Athena

During Metabase setup:

Database: Amazon Athena

Region: eu-north-1

S3 Staging Directory:

s3://athena-query-results-nithinraaj-eu-north-1/


Workgroup: primary

Authentication: Use environment variables (already provided via .env)

After connection:

Sync schema

Select ecommerce_gold tables

6ï¸âƒ£ View Dashboards

Once connected:

http://localhost:3000/dashboard


You will see:

Orders Daily Summary

Payments Summary

User Activity Metrics

âœ… Final Result

âœ” Real-time ingestion via Kafka
âœ” Bronze â†’ Silver â†’ Gold data lake
âœ” Orchestrated using Airflow
âœ” Serverless analytics with Athena
âœ” BI dashboards using Metabase

âš ï¸ Important Notes

Never commit .env files

Stop services when not in use to avoid AWS cost

Glue Crawlers are deferrable to prevent duplicate runs

ğŸ§  Resume Value

This project demonstrates:

Real-world data lake architecture

Production-style orchestration

Cost-aware AWS usage

End-to-end ownership (ingest â†’ BI)

---

## ğŸ’¡ Why This Architecture?

- **Scalable** â€“ Event-driven & serverless analytics
- **Cost-efficient** â€“ No always-on clusters
- **Production-aligned** â€“ Industry-standard data lake pattern
- **Resume-ready** â€“ Real-world tools & design decisions

## ğŸ‘¤ Author

**Nithinraaj**  
Software Engineer | Backend & Data Engineering  
Focused on scalable, real-world system design

---

â­ If this project helped you understand modern data platforms, give it a star!

